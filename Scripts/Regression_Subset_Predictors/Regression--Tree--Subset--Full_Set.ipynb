{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries \n",
    "# make sure that tree_functions_2.py is in the same directory as this notebook\n",
    "\n",
    "from tree_functions_2 import *\n",
    "\n",
    "# calling the appropriate tools for classification\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change figure configurations\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "font = {'size':16}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "matplotlib.rc('figure', figsize = (5.0, 5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the order of the trees \n",
    "\n",
    "n = 12\n",
    "\n",
    "l = int(np.ceil(n / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a random seed\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Tree List and the Associated Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the list of all non-isomorphic trees of order n\n",
    "\n",
    "Tree_List = list(nx.nonisomorphic_trees(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes for trees (0 is path-like and 1 is star-like)\n",
    "\n",
    "classes = [0, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation-based total ordering on Tree_List\n",
    "\n",
    "total_tree_evaluation_list = get_total_list_evaluation_based(Tree_List, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying the trees Tree_List as 'path-like' and 'star-like'\n",
    "# based on the total ordering\n",
    "\n",
    "for j in range(len(total_tree_evaluation_list)):\n",
    "    \n",
    "    if j < len(total_tree_evaluation_list)/2:\n",
    "        \n",
    "        total_tree_evaluation_list[j].append(classes[0])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        total_tree_evaluation_list[j].append(classes[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame containing the following graph statistics:\n",
    "# log_{10}(P(.;2,1)), radius,  diameter, degree centrality, \n",
    "# closeness centrality, between centrality,\n",
    "# Stirling Numbers of the First Kind for Trees, number of leaves, and class \n",
    "\n",
    "df = []\n",
    "\n",
    "for i in range(len(total_tree_evaluation_list)):\n",
    "    \n",
    "    x = total_tree_evaluation_list[i]\n",
    "        \n",
    "    df.append([np.log10(float(x[0])), nx.radius(x[1]), nx.diameter(x[1]),\n",
    "               get_degree_centrality(x[1]),\n",
    "               get_closeness_centrality(x[1]),\n",
    "               get_betweenness_centrality(x[1]),\n",
    "               get_stirling_trees(x[1], n),\n",
    "               get_leaf_number(x[1]),\n",
    "               x[3]])\n",
    "    \n",
    "df = pd.DataFrame(df, columns = ['Log_Dist', 'Rad', 'Diam', 'Deg_Cent', \n",
    "                                 'Cls_Cent', 'Btw_Cent', 'Stirling', 'Leaf_Num', 'Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Splitting the Tree List to Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, [4, 5, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(np.array(list(df.loc[:, 'Stirling']))).iloc[:, 0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a random seed\n",
    "\n",
    "random_state =  np.random.RandomState(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decision tree\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state = random_state)\n",
    "\n",
    "# DecisionTreeRegressor(*, criterion='squared_error', splitter='best', \n",
    "#                       max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "#                       min_weight_fraction_leaf=0.0, max_features=None, \n",
    "#                       random_state=None, max_leaf_nodes=None, \n",
    "#                       min_impurity_decrease=0.0, ccp_alpha=0.0)\n",
    "\n",
    "dtr.fit(X_train, y_train)\n",
    "\n",
    "y_train_dtr_pred = dtr.predict(X_train)\n",
    "\n",
    "y_test_dtr_pred = dtr.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train Score:', dtr.score(X_train, y_train))\n",
    "\n",
    "print('Test Score:', dtr.score(X_test, y_test))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train EVS:', explained_variance_score(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test EVS:', explained_variance_score(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "\n",
    "#tree_data = tree.export_graphviz(dtr, out_file = None) \n",
    "\n",
    "#graph = graphviz.Source(tree_data) \n",
    "\n",
    "#graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n",
    "\n",
    "path = dtr.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "ax.set_xlabel('effective alpha')\n",
    "\n",
    "ax.set_ylabel('total impurity of leaves')\n",
    "\n",
    "ax.set_title('Total Impurity vs effective alpha for training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrs = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    \n",
    "    dtr = DecisionTreeRegressor(random_state = random_state, ccp_alpha = ccp_alpha)\n",
    "    \n",
    "    dtr.fit(X_train, y_train)\n",
    "    \n",
    "    dtrs.append(dtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrs = dtrs[:-1]\n",
    "\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [dtr.tree_.node_count for dtr in dtrs]\n",
    "\n",
    "depth = [dtr.tree_.max_depth for dtr in dtrs]\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].plot(ccp_alphas, node_counts, marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "ax[0].set_xlabel('alpha')\n",
    "\n",
    "ax[0].set_ylabel('number of nodes')\n",
    "\n",
    "ax[0].set_title('Number of nodes vs alpha')\n",
    "\n",
    "ax[1].plot(ccp_alphas, depth, marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "ax[1].set_xlabel('alpha')\n",
    "\n",
    "ax[1].set_ylabel('depth of tree')\n",
    "\n",
    "ax[1].set_title('Depth vs alpha')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [dtr.score(X_train, y_train) for dtr in dtrs]\n",
    "\n",
    "test_scores = [dtr.score(X_test, y_test) for dtr in dtrs]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel('alpha')\n",
    "\n",
    "ax.set_ylabel('accuracy')\n",
    "\n",
    "ax.set_title('Accuracy vs alpha for training and testing sets')\n",
    "\n",
    "ax.plot(ccp_alphas, train_scores, marker = 'o', label = 'train', drawstyle = 'steps-post')\n",
    "\n",
    "ax.plot(ccp_alphas, test_scores, marker = 'o', label = 'test', drawstyle = 'steps-post')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree with pruning\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state = random_state, ccp_alpha = 1)\n",
    "\n",
    "# DecisionTreeRegressor(*, criterion=splitter='best', \n",
    "#                       max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "#                       min_weight_fraction_leaf=0.0, max_features=None, \n",
    "#                       random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "#                       class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "dtr.fit(X_train, y_train)\n",
    "\n",
    "y_train_dtr_pred = dtr.predict(X_train)\n",
    "\n",
    "y_test_dtr_pred = dtr.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train Score:', dtr.score(X_train, y_train))\n",
    "\n",
    "print('Test Score:', dtr.score(X_test, y_test))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train EVS:', explained_variance_score(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test EVS:', explained_variance_score(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "\n",
    "#tree_data = tree.export_graphviz(dtr, out_file = None) \n",
    "\n",
    "#graph = graphviz.Source(tree_data) \n",
    "\n",
    "#graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_reg(X_train, X_test, y_train, y_test, prune_list = np.zeros(shape(y_train)[1] - 1), prune = False):   \n",
    "    \n",
    "    for m in range(shape(y_train)[1] - 1):\n",
    "    \n",
    "        y_train_trunc = np.ravel(np.array(list(y_train.iloc[:, m])))\n",
    "\n",
    "        y_test_trunc = np.ravel(np.array(list(y_test.iloc[:, m])))\n",
    "    \n",
    "        dtr = DecisionTreeRegressor(random_state = random_state, ccp_alpha = prune_list[m] * prune)\n",
    "        \n",
    "        dtr.fit(X_train, y_train_trunc)\n",
    "\n",
    "        y_pred_train = dtr.predict(X_train)\n",
    "\n",
    "        y_pred_train_rounded = [np.rint(y_pred_train[i]) for i in range(len(y_pred_train))]\n",
    "\n",
    "        y_pred_test = dtr.predict(X_test)\n",
    "\n",
    "        y_pred_test_rounded = [np.rint(y_pred_test[i]) for i in range(len(y_pred_test))]\n",
    "        \n",
    "        if prune == False:\n",
    "            \n",
    "            path = dtr.cost_complexity_pruning_path(X_train, y_train_trunc)\n",
    "            \n",
    "            ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "        \n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            ax.plot(ccp_alphas[:-1], impurities[:-1], marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "            ax.set_xlabel('effective alpha')\n",
    "\n",
    "            ax.set_ylabel('total impurity of leaves')\n",
    "\n",
    "            ax.set_title('Total Impurity vs effective alpha for training set')\n",
    "        \n",
    "            dtrs = []\n",
    "\n",
    "            for ccp_alpha in ccp_alphas:\n",
    "    \n",
    "                dtr = DecisionTreeRegressor(random_state = random_state, ccp_alpha = ccp_alpha)\n",
    "    \n",
    "                dtr.fit(X_train, y_train_trunc)\n",
    "    \n",
    "                dtrs.append(dtr)\n",
    "        \n",
    "            dtrs = dtrs[:-1]\n",
    "\n",
    "            ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "            node_counts = [dtr.tree_.node_count for dtr in dtrs]\n",
    "\n",
    "            depth = [dtr.tree_.max_depth for dtr in dtrs]\n",
    "\n",
    "            fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "            ax[0].plot(ccp_alphas, node_counts, marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "            ax[0].set_xlabel('alpha')\n",
    "\n",
    "            ax[0].set_ylabel('number of nodes')\n",
    "\n",
    "            ax[0].set_title('Number of nodes vs alpha')\n",
    "\n",
    "            ax[1].plot(ccp_alphas, depth, marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "            ax[1].set_xlabel('alpha')\n",
    "\n",
    "            ax[1].set_ylabel('depth of tree')\n",
    "\n",
    "            ax[1].set_title('Depth vs alpha')\n",
    "\n",
    "            fig.tight_layout()\n",
    "        \n",
    "            train_scores = [dtr.score(X_train, y_train_trunc) for dtr in dtrs]\n",
    "\n",
    "            test_scores = [dtr.score(X_test, y_test_trunc) for dtr in dtrs]\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            ax.set_xlabel('alpha')\n",
    "\n",
    "            ax.set_ylabel('accuracy')\n",
    "\n",
    "            ax.set_title('Accuracy vs alpha for training and testing sets')\n",
    "\n",
    "            ax.plot(ccp_alphas, train_scores, marker = 'o', label = 'train', drawstyle = 'steps-post')\n",
    "\n",
    "            ax.plot(ccp_alphas, test_scores, marker = 'o', label = 'test', drawstyle = 'steps-post')\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        DF.append(['dtr', 1 * prune, m + l,\n",
    "                   round(dtr.score(X_train, y_train_trunc), 5),\n",
    "                   round(dtr.score(X_test, y_test_trunc), 5),\n",
    "                   round(explained_variance_score(y_train_trunc, y_pred_train), 5),\n",
    "                   round(explained_variance_score(y_test_trunc, y_pred_test), 5)])\n",
    "        \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt_reg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt_reg(X_train, X_test, y_train, y_test, [0.002, 0.5, 2, 2, 1], prune = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra tree\n",
    "\n",
    "etr = ExtraTreeRegressor(random_state = random_state)\n",
    "\n",
    "# ExtraTreeRegressor(*, criterion='squared_error', splitter='random', \n",
    "#                    max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "#                    min_weight_fraction_leaf=0.0, max_features=1.0, \n",
    "#                    random_state=None, min_impurity_decrease=0.0, \n",
    "#                    max_leaf_nodes=None, ccp_alpha=0.0)\n",
    "\n",
    "etr.fit(X_train, y_train)\n",
    "\n",
    "y_train_etr_pred = etr.predict(X_train)\n",
    "\n",
    "y_test_etr_pred = etr.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train Score:', etr.score(X_train, y_train))\n",
    "\n",
    "print('Test Score:', etr.score(X_test, y_test))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train EVS:', explained_variance_score(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test EVS:', explained_variance_score(y_test, y_test_dtr_pred))\n",
    "\n",
    "#print('--------')\n",
    "\n",
    "#tree_data = tree.export_graphviz(dtr, out_file = None) \n",
    "\n",
    "#graph = graphviz.Source(tree_data) \n",
    "\n",
    "#graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = etr.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "ax.set_xlabel('effective alpha')\n",
    "\n",
    "ax.set_ylabel('total impurity of leaves')\n",
    "\n",
    "ax.set_title('Total Impurity vs effective alpha for training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etrs = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    \n",
    "    etr = ExtraTreeRegressor(random_state = random_state, ccp_alpha = ccp_alpha)\n",
    "    \n",
    "    etr.fit(X_train, y_train)\n",
    "    \n",
    "    etrs.append(etr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etrs = etrs[:-1]\n",
    "\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [etr.tree_.node_count for etr in etrs]\n",
    "\n",
    "depth = [etr.tree_.max_depth for etr in etrs]\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].plot(ccp_alphas, node_counts, marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "ax[0].set_xlabel('alpha')\n",
    "\n",
    "ax[0].set_ylabel('number of nodes')\n",
    "\n",
    "ax[0].set_title('Number of nodes vs alpha')\n",
    "\n",
    "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle='steps-post')\n",
    "\n",
    "ax[1].set_xlabel('alpha')\n",
    "\n",
    "ax[1].set_ylabel('depth of tree')\n",
    "\n",
    "ax[1].set_title('Depth vs alpha')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [etr.score(X_train, y_train) for etr in etrs]\n",
    "\n",
    "test_scores = [etr.score(X_test, y_test) for etr in etrs]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel('alpha')\n",
    "\n",
    "ax.set_ylabel('accuracy')\n",
    "\n",
    "ax.set_title('Accuracy vs alpha for training and testing sets')\n",
    "\n",
    "ax.plot(ccp_alphas, train_scores, marker = 'o', label = 'train', drawstyle = 'steps-post')\n",
    "\n",
    "ax.plot(ccp_alphas, test_scores, marker = 'o', label = 'test', drawstyle = 'steps-post')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra tree with pruning\n",
    "\n",
    "etr = ExtraTreeRegressor(random_state = random_state, ccp_alpha = 0.5)\n",
    "\n",
    "# ExtraTreeRegressor(*, criterion='squared_error', splitter='random', \n",
    "#                    max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "#                    min_weight_fraction_leaf=0.0, max_features=1.0, \n",
    "#                    random_state=None, min_impurity_decrease=0.0, \n",
    "#                    max_leaf_nodes=None, ccp_alpha=0.0)\n",
    "\n",
    "etr.fit(X_train, y_train)\n",
    "\n",
    "y_train_etr_pred = etr.predict(X_train)\n",
    "\n",
    "y_test_etr_pred = etr.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train Score:', etr.score(X_train, y_train))\n",
    "\n",
    "print('Test Score:', etr.score(X_test, y_test))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train EVS:', explained_variance_score(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test EVS:', explained_variance_score(y_test, y_test_dtr_pred))\n",
    "\n",
    "#print('--------')\n",
    "\n",
    "#tree_data = tree.export_graphviz(etr, out_file = None) \n",
    "\n",
    "#graph = graphviz.Source(tree_data) \n",
    "\n",
    "#graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def et_reg(X_train, X_test, y_train, y_test, prune_list = np.zeros(shape(y_train)[1] - 1), prune = False):\n",
    "    \n",
    "    for m in range(shape(y_train)[1] - 1):\n",
    "    \n",
    "        y_train_trunc = np.ravel(np.array(list(y_train.iloc[:, m])))\n",
    "\n",
    "        y_test_trunc = np.ravel(np.array(list(y_test.iloc[:, m])))\n",
    "    \n",
    "        etr = ExtraTreeRegressor(random_state = random_state, ccp_alpha = prune_list[m] * prune)\n",
    "        \n",
    "        etr.fit(X_train, y_train_trunc)\n",
    "\n",
    "        y_pred_train = etr.predict(X_train)\n",
    "\n",
    "        y_pred_train_rounded = [np.rint(y_pred_train[i]) for i in range(len(y_pred_train))]\n",
    "\n",
    "        y_pred_test = etr.predict(X_test)\n",
    "\n",
    "        y_pred_test_rounded = [np.rint(y_pred_test[i]) for i in range(len(y_pred_test))]\n",
    "        \n",
    "        if prune == False:\n",
    "            \n",
    "            path = etr.cost_complexity_pruning_path(X_train, y_train_trunc)\n",
    "            \n",
    "            ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "        \n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            ax.plot(ccp_alphas[:-1], impurities[:-1], marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "            ax.set_xlabel('effective alpha')\n",
    "\n",
    "            ax.set_ylabel('total impurity of leaves')\n",
    "\n",
    "            ax.set_title('Total Impurity vs effective alpha for training set')\n",
    "        \n",
    "            etrs = []\n",
    "\n",
    "            for ccp_alpha in ccp_alphas:\n",
    "    \n",
    "                etr = DecisionTreeRegressor(random_state = random_state, ccp_alpha = ccp_alpha)\n",
    "    \n",
    "                etr.fit(X_train, y_train_trunc)\n",
    "    \n",
    "                etrs.append(etr)\n",
    "        \n",
    "            etrs = etrs[:-1]\n",
    "\n",
    "            ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "            node_counts = [etr.tree_.node_count for etr in etrs]\n",
    "\n",
    "            depth = [etr.tree_.max_depth for etr in etrs]\n",
    "\n",
    "            fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "            ax[0].plot(ccp_alphas, node_counts, marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "            ax[0].set_xlabel('alpha')\n",
    "\n",
    "            ax[0].set_ylabel('number of nodes')\n",
    "\n",
    "            ax[0].set_title('Number of nodes vs alpha')\n",
    "\n",
    "            ax[1].plot(ccp_alphas, depth, marker = 'o', drawstyle = 'steps-post')\n",
    "\n",
    "            ax[1].set_xlabel('alpha')\n",
    "\n",
    "            ax[1].set_ylabel('depth of tree')\n",
    "\n",
    "            ax[1].set_title('Depth vs alpha')\n",
    "\n",
    "            fig.tight_layout()\n",
    "        \n",
    "            train_scores = [etr.score(X_train, y_train_trunc) for etr in etrs]\n",
    "\n",
    "            test_scores = [etr.score(X_test, y_test_trunc) for etr in etrs]\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            ax.set_xlabel('alpha')\n",
    "\n",
    "            ax.set_ylabel('accuracy')\n",
    "\n",
    "            ax.set_title('Accuracy vs alpha for training and testing sets')\n",
    "\n",
    "            ax.plot(ccp_alphas, train_scores, marker = 'o', label = 'train', drawstyle = 'steps-post')\n",
    "\n",
    "            ax.plot(ccp_alphas, test_scores, marker = 'o', label = 'test', drawstyle = 'steps-post')\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        DF.append(['etr', 1 * prune, m + l,\n",
    "                   round(etr.score(X_train, y_train_trunc), 5),\n",
    "                   round(etr.score(X_test, y_test_trunc), 5),\n",
    "                   round(explained_variance_score(y_train_trunc, y_pred_train), 5),\n",
    "                   round(explained_variance_score(y_test_trunc, y_pred_test), 5)])\n",
    "        \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "et_reg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "et_reg(X_train, X_test, y_train, y_test, prune_list = [0.002, 0.2, 2, 2, 1], prune = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging\n",
    "\n",
    "br = BaggingRegressor(random_state = random_state)\n",
    "\n",
    "# BaggingRegressor(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, \n",
    "#                  bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, \n",
    "#                 n_jobs=None, random_state=None, verbose=0)\n",
    "\n",
    "br.fit(X_train, y_train)\n",
    "\n",
    "y_train_br_pred = br.predict(X_train)\n",
    "\n",
    "y_test_br_pred = br.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_br_pred))\n",
    "\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_br_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train Score:', br.score(X_train, y_train))\n",
    "\n",
    "print('Test Score:', br.score(X_test, y_test))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train EVS:', explained_variance_score(y_train, y_train_br_pred))\n",
    "\n",
    "print('Test EVS:', explained_variance_score(y_test, y_test_br_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_reg(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    for m in range(shape(y_train)[1] - 1):\n",
    "    \n",
    "        y_train_trunc = np.ravel(np.array(list(y_train.iloc[:, m])))\n",
    "\n",
    "        y_test_trunc = np.ravel(np.array(list(y_test.iloc[:, m])))\n",
    "    \n",
    "        br = BaggingRegressor(random_state = random_state)\n",
    "        \n",
    "        br.fit(X_train, y_train_trunc)\n",
    "\n",
    "        y_pred_train = br.predict(X_train)\n",
    "\n",
    "        y_pred_train_rounded = [np.rint(y_pred_train[i]) for i in range(len(y_pred_train))]\n",
    "\n",
    "        y_pred_test = br.predict(X_test)\n",
    "\n",
    "        y_pred_test_rounded = [np.rint(y_pred_test[i]) for i in range(len(y_pred_test))]\n",
    "\n",
    "        DF.append(['br', 'na', m + l,\n",
    "                   round(br.score(X_train, y_train_trunc), 5),\n",
    "                   round(br.score(X_test, y_test_trunc), 5),\n",
    "                   round(explained_variance_score(y_train_trunc, y_pred_train), 5),\n",
    "                   round(explained_variance_score(y_test_trunc, y_pred_test), 5)])\n",
    "        \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_reg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 50, random_state = random_state)\n",
    "\n",
    "# RandomForestRegressor(n_estimators=100, *, criterion=max_depth=None, \n",
    "#                       min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "#                       max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "#                       bootstrap=True, oob_score=False, n_jobs=None, \n",
    "#                       random_state=None, verbose=0, warm_start=False, \n",
    "#                       class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "y_train_rfr_pred = rfr.predict(X_train)\n",
    "\n",
    "y_test_rfr_pred = rfr.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train Score:', rfr.score(X_train, y_train))\n",
    "\n",
    "print('Test Score:', rfr.score(X_test, y_test))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train EVS:', explained_variance_score(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test EVS:', explained_variance_score(y_test, y_test_dtr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with pruning\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 50, random_state = random_state, ccp_alpha = 0.5)\n",
    "\n",
    "# RandomForestRegressor(n_estimators=100, *, criterion=max_depth=None, \n",
    "#                       min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "#                       max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "#                       bootstrap=True, oob_score=False, n_jobs=None, \n",
    "#                       random_state=None, verbose=0, warm_start=False, \n",
    "#                       class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "y_train_rfr_pred = rfr.predict(X_train)\n",
    "\n",
    "y_test_rfr_pred = rfr.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train Score:', rfr.score(X_train, y_train))\n",
    "\n",
    "print('Test Score:', rfr.score(X_test, y_test))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train EVS:', explained_variance_score(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test EVS:', explained_variance_score(y_test, y_test_dtr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_reg(X_train, X_test, y_train, y_test, prune_list = np.zeros(shape(y_train)[1] - 1), prune = False):\n",
    "    \n",
    "    for m in range(shape(y_train)[1] - 1):\n",
    "    \n",
    "        y_train_trunc = np.ravel(np.array(list(y_train.iloc[:, m])))\n",
    "\n",
    "        y_test_trunc = np.ravel(np.array(list(y_test.iloc[:, m])))\n",
    "    \n",
    "        rfr = RandomForestRegressor(n_estimators = 50, random_state = random_state, \n",
    "                                    ccp_alpha = prune_list[m] * prune)\n",
    "        \n",
    "        rfr.fit(X_train, y_train_trunc)\n",
    "\n",
    "        y_pred_train = rfr.predict(X_train)\n",
    "\n",
    "        y_pred_train_rounded = [np.rint(y_pred_train[i]) for i in range(len(y_pred_train))]\n",
    "\n",
    "        y_pred_test = rfr.predict(X_test)\n",
    "\n",
    "        y_pred_test_rounded = [np.rint(y_pred_test[i]) for i in range(len(y_pred_test))]\n",
    "\n",
    "        DF.append(['rfr', 1 * prune, m + l,\n",
    "                   round(rfr.score(X_train, y_train_trunc), 5),\n",
    "                   round(rfr.score(X_test, y_test_trunc), 5),\n",
    "                   round(explained_variance_score(y_train_trunc, y_pred_train), 5),\n",
    "                   round(explained_variance_score(y_test_trunc, y_pred_test), 5)])\n",
    "        \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_reg(X_train, X_test, y_train, y_test, prune_list = [0.002, 0.4, 2, 2, 1], prune = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees  with Gini Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra trees with gini\n",
    "\n",
    "etsr = ExtraTreesRegressor(n_estimators = 50, random_state = random_state)\n",
    "\n",
    "# ExtraTreesRegressor(n_estimators=100, *, criterion=max_depth=None, \n",
    "#                     min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "#                     max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "#                     bootstrap=False, oob_score=False, n_jobs=None, \n",
    "#                     random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "#                     ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "etsr.fit(X_train, y_train)\n",
    "\n",
    "y_train_etsr_pred = etsr.predict(X_train)\n",
    "\n",
    "y_test_etsr_pred = etsr.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train Score:', etsr.score(X_train, y_train))\n",
    "\n",
    "print('Test Score:', etsr.score(X_test, y_test))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train EVS:', explained_variance_score(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test EVS:', explained_variance_score(y_test, y_test_dtr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra trees with pruning\n",
    "\n",
    "etsr = ExtraTreesRegressor(n_estimators = 50, random_state = random_state, ccp_alpha = 0.5)\n",
    "\n",
    "# ExtraTreesRegressor(n_estimators=100, *, criterion=max_depth=None, \n",
    "#                     min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "#                     max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "#                     bootstrap=False, oob_score=False, n_jobs=None, \n",
    "#                     random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "#                     ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "etsr.fit(X_train, y_train)\n",
    "\n",
    "y_train_etsr_pred = etsr.predict(X_train)\n",
    "\n",
    "y_test_etsr_pred = etsr.predict(X_test)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_dtr_pred))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train Score:', etsr.score(X_train, y_train))\n",
    "\n",
    "print('Test Score:', etsr.score(X_test, y_test))\n",
    "\n",
    "print('--------')\n",
    "\n",
    "print('Train EVS:', explained_variance_score(y_train, y_train_dtr_pred))\n",
    "\n",
    "print('Test EVS:', explained_variance_score(y_test, y_test_dtr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ets_reg(X_train, X_test, y_train, y_test, prune_list = np.zeros(shape(y_train)[1] - 1), prune = False):\n",
    "    \n",
    "    for m in range(shape(y_train)[1] - 1):\n",
    "    \n",
    "        y_train_trunc = np.ravel(np.array(list(y_train.iloc[:, m])))\n",
    "\n",
    "        y_test_trunc = np.ravel(np.array(list(y_test.iloc[:, m])))\n",
    "    \n",
    "        etsr = ExtraTreesRegressor(n_estimators = 50, random_state = random_state,\n",
    "                                   ccp_alpha = prune_list[m] * prune)\n",
    "        \n",
    "        etsr.fit(X_train, y_train_trunc)\n",
    "\n",
    "        y_pred_train = etsr.predict(X_train)\n",
    "\n",
    "        y_pred_train_rounded = [np.rint(y_pred_train[i]) for i in range(len(y_pred_train))]\n",
    "\n",
    "        y_pred_test = etsr.predict(X_test)\n",
    "\n",
    "        y_pred_test_rounded = [np.rint(y_pred_test[i]) for i in range(len(y_pred_test))]\n",
    "\n",
    "        DF.append(['etsr', 1 * prune, m + l,\n",
    "                   round(etsr.score(X_train, y_train_trunc), 5),\n",
    "                   round(etsr.score(X_test, y_test_trunc), 5),\n",
    "                   round(explained_variance_score(y_train_trunc, y_pred_train), 5),\n",
    "                   round(explained_variance_score(y_test_trunc, y_pred_test), 5)])\n",
    "        \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_reg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ets_reg(X_train, X_test, y_train, y_test, prune_list = [0.002, 0.2, 2, 2, 1], prune = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2 = pd.DataFrame(np.array(DF), columns = ['Method', 'Pruning', 'k', 'Train_Score',\n",
    "                                            'Test_Score', 'Train_EVS', 'Test_EVS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.to_csv('Regression--Tree--Subset--Full_Set.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
